include required(classpath("application"))
###### FH Slurm Backend, with call caching, with docker/singularity
database {
  profile = "slick.jdbc.MySQLProfile$"
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    connectionTimeout = 5000
  }
}
workflow-options {
    # save all workflow logs to refer back to
    workflow-log-temporary = false
}
akka.http.server.request-timeout = 30s
call-caching {
  # Allows re-use of existing results for jobs you've already run (default: false)
  enabled = true

  # Whether to invalidate a cache result forever if we cannot reuse them. Disable this if you expect some cache copies
  # to fail for external reasons which should not invalidate the cache (e.g. auth differences between users):
  # (default: true)
  invalidate-bad-cache-results = true
}
aws {
  application-name = "cromwell"
  auths = [
    {
      name = "default"
      scheme = "default"
    }
  ]
   region = "us-west-2" 
   ## uses region from ~/.aws/config set by aws configure command,
   ##                     or us-east-1 by default
} 
engine {
  filesystems { s3 { auth = "default" } }
}
# Backend and filesystem
backend {
  default = gizmo
  providers {
    gizmo {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        glob-link-command = "ln -sL GLOB_PATTERN GLOB_DIRECTORY"
        # For BeeGFS so softlink is used instead of hardlink

        runtime-attributes = """
        Int cpu = 1
        String walltime = "18:00:00"
        Int memory_mb = 2000
        String partition = "campus-new"
        String? docker
        String? modules = ""
        """

        submit = """
            set -e
            source /app/lmod/lmod/init/bash
            module use /app/modules/all
            module purge
            
            module load ${modules}
            
            sbatch \
              --wait \
              --partition=${partition} \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              --cpus-per-task=${cpu} \
              --mem=${memory_mb} \
              --time=${walltime} \
              --wrap "/bin/bash ${script}"
        """
        submit-docker = """
            set -e 
            source /app/lmod/lmod/init/bash
            module use /app/modules/all
            module purge

            # Ensure singularity is loaded if it's installed as a module
            module load Singularity/3.5.3
            
            LOCK_FILE=$CACHE_DIR/singularity_pull_flock
            # Create an exclusive filelock with flock. --verbose is useful for 
            # for debugging, as is the echo command. These show up in `stdout.submit`.
            flock --verbose --exclusive --timeout 900 $LOCK_FILE \
            singularity exec --containall docker://${docker} \
            echo "successfully pulled ${docker}!"

            # Submit the script to SLURM
            sbatch \
              --wait \
              --partition=${partition} \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              --cpus-per-task=${cpu} \
              --mem=${memory_mb} \
              --time=${walltime} \
              --wrap "singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}"
        """
       filesystems {
          local {
            localization: [
              ## for local SLURM, hardlink doesn't work. Options for this and caching: , "soft-link" , "hard-link", "copy"
              "soft-link", "copy"
            ]
            ## call caching config relating to the filesystem side
            caching {
              # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
              duplication-strategy: [
                "soft-link", "copy"
              ]
              # Possible values: file, path, path+modtime
              # "file" will compute an md5 hash of the file content.
              # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
              # in order to allow for the original file path to be hashed.
              # "path+modtime" will compute an md5 hash of the file path and the last modified time. The same conditions as for "path" apply here.
              # Default: file
              hashing-strategy: "path+modtime"

              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
              # Default: false
              check-sibling-md5: false
            }
          }
          s3 { auth = "default" }
        }
        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }
  }
}
