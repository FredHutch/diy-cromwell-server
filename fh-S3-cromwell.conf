include required(classpath("application"))
services {  MetadataService {
    class = "cromwell.services.metadata.impl.MetadataServiceActor"
    config {  metadata-read-row-number-safety-threshold = 2000000  }  }  }

database {
  profile = "slick.jdbc.MySQLProfile$"
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    connectionTimeout = 5000
  }
}
webservice {
  binding-timeout = 10s
}
workflow-heartbeats {
  heartbeat-interval: 2 minutes
  ttl: 10 minutes
  write-failure-shutdown-duration: 5 minutes
  write-batch-size: 10000
  write-threshold: 10000
}
system {
   file-hash-cache = true
   input-read-limits {
       tsv = 1073741823
       object = 1073741823
       string = 1073741823
       lines = 1073741823
       json = 1073741823
   }
    io {
      number-of-requests = 100000
      per = 5 seconds # normally 100 seconds
      number-of-attempts = 10
    }
}
workflow-options {
    # save all workflow logs to refer back to
    workflow-log-temporary = false
}
akka.http.server.request-timeout = 60s
call-caching {
  # Allows re-use of existing results for jobs you've already run (default: false)
  enabled = true

  # Whether to invalidate a cache result forever if we cannot reuse them. Disable this if you expect some cache copies
  # to fail for external reasons which should not invalidate the cache (e.g. auth differences between users):
  # (default: true)
  invalidate-bad-cache-results = true
}
aws {
  application-name = "cromwell"
  auths = [
    {
      name = "default"
      scheme = "default"
    }
  ]
   region = "us-west-2" 
} 
engine {
  filesystems { 
    local {
      localization: [
           "soft-link", "copy" # See SLURM backend for definitions. 
          ]
        }
    s3 { auth = "default" }
    } 
  }
}

# Backend and filesystem
backend {
  default = "gizmo"
  providers {
    gizmo {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        glob-link-command = "ln -sL GLOB_PATTERN GLOB_DIRECTORY"
        # For BeeGFS so softlink is used instead of hardlink
        concurrent-job-limit = 5000
        runtime-attributes = """
        Int cpu = 1
        String walltime = "18:00:00"
        Int memory_mb = 2000
        String partition = "campus-new"
        String? docker
        String? modules = ""
        """

        submit = """
            set -e
            source /app/lmod/lmod/init/bash
            module use /app/modules/all
            module purge
            
            module load ${modules}
            
            sbatch \
              --partition=${partition} \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              --cpus-per-task=${cpu} \
              --time=${walltime} \
              --wrap "/bin/bash ${script}"
        """
        submit-docker = """
            set -e 
            source /app/lmod/lmod/init/bash
            module use /app/modules/all
            module purge

            # Ensure singularity is loaded if it's installed as a module
            module load Singularity/3.5.3
            # Build the Docker image into a singularity image
            DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
            # The image will live together with all the other images to force "caching" of the .sif files themselves - note, always use docker hub tags!!!
            IMAGE=$SINGULARITYCACHEDIR/$DOCKER_NAME.sif

            if [ ! -f $IMAGE ]; then  # If we already have the image, skip everything
                singularity pull $IMAGE docker://${docker}
            fi   

            # Submit the script to SLURM
            sbatch \
              --partition=${partition} \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              --cpus-per-task=${cpu} \
              --time=${walltime} \
              --wrap "singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}"
        """
       filesystems {
          local {
            localization: [
              ## for local SLURM, hardlink doesn't work. Options for this and caching: , "soft-link" , "hard-link", "copy"
              "soft-link", "copy"
            ]
            ## call caching config relating to the filesystem side
            caching {
              # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:  "hard-link", "soft-link", "copy", "cached-copy".
              duplication-strategy: [
                "soft-link", "copy"
              ]
              #cached-copy An experimental feature. This copies files to a file cache in 
              # <workflow_root>/cached-inputs and then hard links them in the <call_dir>/inputs 
              # directory.  cached-copy is intended for a shared filesystem that runs on multiple 
              # physical disks, where docker containers are used. Hard-links don't work between 
              # different physical disks and soft-links don't work with docker. 
              # Copying uses a lot of space if a multitude of tasks use the same input. 
              # cached-copy copies the file only once to the physical disk containing the 
              # <workflow_root> and then uses hard links for every task that needs the input file. This can save a lot of space.
              
              # Possible values: md5, xxh64, fingerprint, path, path+modtime
              # For extended explanation check: https://cromwell.readthedocs.io/en/stable/Configuring/#call-caching
              # "md5" will compute an md5 hash of the file content.
              # "xxh64" will compute an xxh64 hash of the file content. Much faster than md5
              # "fingerprint" will take last modified time, size and hash the first 10 mb with xxh64 to create a file fingerprint.
              # This strategy will only be effective if the duplication-strategy (above) is set to "hard-link", as copying changes the last modified time.
              # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
              # in order to allow for the original file path to be hashed.
              # "path+modtime" will compute an md5 hash of the file path and the last modified time. The same conditions as for "path" apply here.
              # Default: "md5"
              hashing-strategy: "path+modtime"

              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
              # Default: false
              check-sibling-md5: false
            }
          }
          s3 { auth = "default" }
        }
        kill = "scancel ${job_id}"
        kill-docker = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }
  }
}
